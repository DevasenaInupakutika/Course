{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning/BigData EE-6973-001-Fall-2016\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span> \n",
    "\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2em;\"> **Autonomous Decision Making for Driverless Cars** </span>  \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.6em;\"> Nicholas Gamez, Nicolas Gallardo </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.6em;\"> *jyi358, hbq774* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {jyi358, hbq774}@my.utsa.edu </span>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Dataset:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The simulation training data can be found in [http://deepdriving.cs.princeton.edu/TORCS_trainset.zip][1]. This directory contains over 50 GB of simluation data of a car with the direction of the steering wheel and surrounding caras and road markings. </span> \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This training data can be found in [http://www.cvlibs.net/datasets/kitti/raw_data.php][2]. This site contains data pertaining to a station wagon platform, the data includes: stereo vision, location, speed, acceleration, meta information,Camera, Camera-to-GPS/IMU, Camera-to-Velodyne. 3D object tracklet labels: cars, trucks, trams, pedestrians, cyclists. This site also let to the below data sets. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This training data can be found in [http://www.rawseeds.org/rs/datasets/view//7][3]. This directory contains data useful for car classification and traversing a campus environment. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This training data can be found in [http://www.robots.ox.ac.uk/NewCollegeData/index.php?n=Main.Downloads#Full][4]. Stereo image data and log data. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This training data can be found in [http://robots.engin.umich.edu/SoftwareData/Ford][5]. Huge data set (>100GB) of both the loop at Ford Research Labs and downtown Dearbourn, Michigan. Dataset contains laserscan, IMU, GPS, image data. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This training data can be found in [http://3dvis.ri.cmu.edu/data-sets/localization/][6]. Lots of visual localization data from CMU. </span>\n",
    "\n",
    "\n",
    "[1]: http://deepdriving.cs.princeton.edu/\n",
    "[2]: http://www.cvlibs.net/datasets/kitti/raw_data.php\n",
    "[3]: http://www.rawseeds.org/rs/datasets/view//7\n",
    "[4]: http://www.robots.ox.ac.uk/NewCollegeData/index.php?n=Main.Downloads#Full\n",
    "[5]: http://robots.engin.umich.edu/SoftwareData/Ford\n",
    "[6]: http://3dvis.ri.cmu.edu/data-sets/localization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Outcome:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Apply deep neural network to determine the best course of action for a car given an endpoint.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Project Definition:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Autonomous Driving has been a hot topic with companies like Google, Uber, and Tesla and they have had some success in applying algorithms to commercial cars. Using Deep Learning techniques and the Tensorflow framework, the goal is to navigate a driverless car through an urban environment. The novely in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation.</span>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The dataset chosen is comprised of a simulation enviroment emulating real life driving situations for training[1]. Feature points such as direction of the steering wheel, location of nearby cars, and specific road markings will be used to determine the best course of action to safely navigate to a desired endpoint. After training, the goal is to apply this alrogithm to an autonomous vehicle instead of more simulation. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The rest of the data will be used to learn different components of driving i.e, car, road sign, pedestrian, cyclist identification, etc. Also as a general supplement to the primary dataset mentioned above. </span>\n",
    "\n",
    "[1]:  C. Chen, A. Seff, A. Kornhauser and J. Xiao, DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving, Proceedings of 15th IEEE International Conference on Computer Vision (ICCV2015) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:220px; \">\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Deep Driving figure from Princeton. We will be using our own code to implement the direct perception method</span>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://deepdriving.cs.princeton.edu/teaser.jpg\" width=\"400\" height=\"100\"/>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
